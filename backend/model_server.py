# file: backend/model_server.py
"""
Model server wrapper.

æ–‡æ¡£ä¼˜å…ˆï¼šContractClassifierServer å°è£…æ¨¡åž‹åŠ è½½ã€é¢„æµ‹ä¸Žçƒ­é‡è½½ã€‚
"""
import os
import sys
import threading
from typing import List, Optional, Tuple

import torch
from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel

# labels ç›´æŽ¥ä»Žä½ æä¾›çš„ä»£ç å¤ç”¨ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
LABELS = [
    'Unhandled Exception (Unchecked Call Return Value): Failing to check the return value of external calls (e.g., send(), call()), which may cause unexpected behavior if the call fails.',
    'Authorization through tx.origin: Using tx.origin for authorization checks, which can be exploited by malicious contracts forwarding transactions.',
    'Reentrancy: Allowing an external contract to re-enter the function before state updates are completed, potentially draining funds.',
    'Arithmetic (Integer Overflow and Underflow): Lack of overflow/underflow checks in arithmetic operations, leading to incorrect results or exploits.',
    'Timestamp Ordering (Transaction Order Dependence): Logic depending on transaction order or block timestamp, which can be manipulated by miners.',
    'Locked Ether: Ether sent to a contract cannot be withdrawn because there is no withdrawal function or self-destruct.',
    'Time Manipulation (Block values as a proxy for time): Directly relying on block.timestamp or block.number as time sources, which miners can slightly alter.'
]

class ContractClassifierServer:
    """
    çº¿ç¨‹å®‰å…¨çš„æ¨¡åž‹åŒ…è£…å™¨ã€‚æ”¯æŒ load_modelã€predictã€reload_modelã€‚
    """
    def __init__(self, base_model_path: str, adapter_path: Optional[str] = None, device: Optional[str] = None):
        self.base_model_path = base_model_path
        self.adapter_path = adapter_path
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.lock = threading.RLock()
        self.model = None
        self.tokenizer = None
        self.loaded = False
        self.last_load_error = None
        # å»¶è¿ŸåŠ è½½ä¸åœ¨ __init__ é‡Œç›´æŽ¥è§¦å‘ï¼Œè°ƒç”¨ load_model å¯åŠ¨åŠ è½½
    def _local_files_only(self):
        # åœ¨æœ‰äº›éƒ¨ç½²ä¼šå¸Œæœ›ä»Žè¿œç¨‹åŠ è½½ï¼ŒçŽ°é˜¶æ®µå¼ºåˆ¶æœ¬åœ°åŠ è½½
        return True

    def load_model(self, base_model_path: Optional[str] = None, adapter_path: Optional[str] = None):
        with self.lock:
            base_path = base_model_path or self.base_model_path
            adapter = adapter_path or self.adapter_path

            print(f"\n[ðŸ§ ] æ­£åœ¨åŠ è½½æ¨¡åž‹ï¼š")
            print(f"     âž¤ Base model: {base_path}")
            print(f"     âž¤ Adapter: {adapter or '(æ— )'}")
            print(f"     âž¤ Device: {self.device}")

            try:
                # --- 1ï¸âƒ£ åŠ è½½é…ç½® ---
                config = AutoConfig.from_pretrained(
                    base_path,
                    num_labels=len(LABELS),
                    problem_type="multi_label_classification",
                    local_files_only=self._local_files_only()
                )

                # --- 2ï¸âƒ£ åŠ è½½åŸºç¡€æ¨¡åž‹ï¼ˆä¸å¿½ç•¥ç»´åº¦ä¸åŒ¹é…ï¼‰---
                base_model = AutoModelForSequenceClassification.from_pretrained(
                    base_path,
                    config=config,
                    local_files_only=self._local_files_only()
                )

                # --- 3ï¸âƒ£ åŠ è½½ adapterï¼ˆå¦‚æžœå­˜åœ¨ï¼‰---
                if adapter and os.path.isdir(adapter):
                    try:
                        model = PeftModel.from_pretrained(
                            base_model,
                            adapter,
                            local_files_only=self._local_files_only()
                        )
                        print("[âœ…] LoRA adapter åŠ è½½æˆåŠŸã€‚")
                    except Exception as e:
                        print(f"[âš ï¸] Adapter åŠ è½½å¤±è´¥ï¼š{e}")
                        print("     âš ï¸ ä½¿ç”¨åŸºç¡€æ¨¡åž‹ç»§ç»­è¿è¡Œï¼ˆæœªåŠ è½½å¾®è°ƒæƒé‡ï¼‰ã€‚")
                        model = base_model
                else:
                    print("[â„¹ï¸] æœªæä¾› adapter æˆ–è·¯å¾„ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡åž‹ã€‚")
                    model = base_model

                # --- 4ï¸âƒ£ è®¾ç½®è®¾å¤‡ ---
                model.to(self.device)
                model.eval()

                # --- 5ï¸âƒ£ åŠ è½½ tokenizer ---
                tokenizer = AutoTokenizer.from_pretrained(base_path, local_files_only=self._local_files_only())
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                # --- 6ï¸âƒ£ æ¸…ç†æ—§æ¨¡åž‹ ---
                try:
                    if getattr(self, "model", None) is not None:
                        del self.model
                        torch.cuda.empty_cache()
                except Exception:
                    pass

                # --- 7ï¸âƒ£ ä¿å­˜æ–°æ¨¡åž‹ ---
                self.model = model
                self.tokenizer = tokenizer
                self.base_model_path = base_path
                self.adapter_path = adapter
                self.loaded = True
                self.last_load_error = None

                print("[âœ…] æ¨¡åž‹åŠ è½½å®Œæˆï¼")
                return True, "loaded"

            except Exception as e:
                self.loaded = False
                self.last_load_error = str(e)
                print(f"[âŒ] æ¨¡åž‹åŠ è½½å¤±è´¥ï¼š{e}")
                return False, str(e)

            

    def predict(self, text: str, threshold: float = 0.5, max_length: int = 512) -> Tuple[List[str], List[float]]:
        """
        è¿”å›ž (matched_labels, probs)
        """
        if not self.loaded:
            raise RuntimeError("Model not loaded: " + (self.last_load_error or "unknown"))
        with self.lock:
            # tokenize
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=max_length
            )
            # move inputs to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                probs = torch.sigmoid(logits).cpu().numpy()[0].tolist()
            matched = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]
            return matched, probs

    def status(self):
        return {
            "loaded": self.loaded,
            "device": self.device,
            "base_model_path": self.base_model_path,
            "adapter_path": self.adapter_path,
            "last_load_error": self.last_load_error
        }
