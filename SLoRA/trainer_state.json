{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 3460,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014450867052023121,
      "grad_norm": 9.76059627532959,
      "learning_rate": 0.000498697539797395,
      "loss": 0.6143,
      "step": 10
    },
    {
      "epoch": 0.028901734104046242,
      "grad_norm": 10.610708236694336,
      "learning_rate": 0.0004972503617945007,
      "loss": 0.5495,
      "step": 20
    },
    {
      "epoch": 0.04335260115606936,
      "grad_norm": 10.012290954589844,
      "learning_rate": 0.0004958031837916063,
      "loss": 0.5515,
      "step": 30
    },
    {
      "epoch": 0.057803468208092484,
      "grad_norm": 17.223119735717773,
      "learning_rate": 0.000494356005788712,
      "loss": 0.3888,
      "step": 40
    },
    {
      "epoch": 0.07225433526011561,
      "grad_norm": 9.764835357666016,
      "learning_rate": 0.0004930535455861071,
      "loss": 0.5046,
      "step": 50
    },
    {
      "epoch": 0.08670520231213873,
      "grad_norm": 13.027482032775879,
      "learning_rate": 0.0004916063675832128,
      "loss": 0.5621,
      "step": 60
    },
    {
      "epoch": 0.10115606936416185,
      "grad_norm": 10.203112602233887,
      "learning_rate": 0.0004901591895803185,
      "loss": 0.4048,
      "step": 70
    },
    {
      "epoch": 0.11560693641618497,
      "grad_norm": 10.944744110107422,
      "learning_rate": 0.000488712011577424,
      "loss": 0.3534,
      "step": 80
    },
    {
      "epoch": 0.13005780346820808,
      "grad_norm": 10.723604202270508,
      "learning_rate": 0.00048726483357452966,
      "loss": 0.2903,
      "step": 90
    },
    {
      "epoch": 0.14450867052023122,
      "grad_norm": 0.8787139058113098,
      "learning_rate": 0.0004858176555716353,
      "loss": 0.5015,
      "step": 100
    },
    {
      "epoch": 0.15895953757225434,
      "grad_norm": 4.498727321624756,
      "learning_rate": 0.000484370477568741,
      "loss": 0.6189,
      "step": 110
    },
    {
      "epoch": 0.17341040462427745,
      "grad_norm": 15.220645904541016,
      "learning_rate": 0.00048292329956584664,
      "loss": 0.4069,
      "step": 120
    },
    {
      "epoch": 0.18786127167630057,
      "grad_norm": 8.373574256896973,
      "learning_rate": 0.00048147612156295225,
      "loss": 0.376,
      "step": 130
    },
    {
      "epoch": 0.2023121387283237,
      "grad_norm": 11.163058280944824,
      "learning_rate": 0.0004800289435600579,
      "loss": 0.3977,
      "step": 140
    },
    {
      "epoch": 0.21676300578034682,
      "grad_norm": 12.10502815246582,
      "learning_rate": 0.00047858176555716356,
      "loss": 0.3853,
      "step": 150
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 3.052644968032837,
      "learning_rate": 0.00047713458755426917,
      "loss": 0.3751,
      "step": 160
    },
    {
      "epoch": 0.24566473988439305,
      "grad_norm": 12.999994277954102,
      "learning_rate": 0.00047568740955137483,
      "loss": 0.5593,
      "step": 170
    },
    {
      "epoch": 0.26011560693641617,
      "grad_norm": 8.26912784576416,
      "learning_rate": 0.00047424023154848044,
      "loss": 0.3849,
      "step": 180
    },
    {
      "epoch": 0.2745664739884393,
      "grad_norm": 0.09857333451509476,
      "learning_rate": 0.0004727930535455861,
      "loss": 0.4273,
      "step": 190
    },
    {
      "epoch": 0.28901734104046245,
      "grad_norm": 14.00342845916748,
      "learning_rate": 0.00047134587554269175,
      "loss": 0.6599,
      "step": 200
    },
    {
      "epoch": 0.30346820809248554,
      "grad_norm": 4.3345160484313965,
      "learning_rate": 0.0004698986975397974,
      "loss": 0.3382,
      "step": 210
    },
    {
      "epoch": 0.3179190751445087,
      "grad_norm": 10.105984687805176,
      "learning_rate": 0.0004684515195369031,
      "loss": 0.3297,
      "step": 220
    },
    {
      "epoch": 0.33236994219653176,
      "grad_norm": 10.759695053100586,
      "learning_rate": 0.0004670043415340087,
      "loss": 0.2441,
      "step": 230
    },
    {
      "epoch": 0.3468208092485549,
      "grad_norm": 4.766557216644287,
      "learning_rate": 0.00046555716353111434,
      "loss": 0.1833,
      "step": 240
    },
    {
      "epoch": 0.36127167630057805,
      "grad_norm": 11.641085624694824,
      "learning_rate": 0.00046410998552822,
      "loss": 0.5006,
      "step": 250
    },
    {
      "epoch": 0.37572254335260113,
      "grad_norm": 15.059743881225586,
      "learning_rate": 0.00046266280752532566,
      "loss": 0.4069,
      "step": 260
    },
    {
      "epoch": 0.3901734104046243,
      "grad_norm": 9.572606086730957,
      "learning_rate": 0.0004612156295224313,
      "loss": 0.4063,
      "step": 270
    },
    {
      "epoch": 0.4046242774566474,
      "grad_norm": 19.063819885253906,
      "learning_rate": 0.00045976845151953687,
      "loss": 0.4172,
      "step": 280
    },
    {
      "epoch": 0.4190751445086705,
      "grad_norm": 10.467048645019531,
      "learning_rate": 0.0004583212735166425,
      "loss": 0.4311,
      "step": 290
    },
    {
      "epoch": 0.43352601156069365,
      "grad_norm": 14.14615249633789,
      "learning_rate": 0.0004568740955137482,
      "loss": 0.4328,
      "step": 300
    },
    {
      "epoch": 0.4479768786127168,
      "grad_norm": 0.26099246740341187,
      "learning_rate": 0.00045542691751085385,
      "loss": 0.3462,
      "step": 310
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 21.971065521240234,
      "learning_rate": 0.0004539797395079595,
      "loss": 0.4275,
      "step": 320
    },
    {
      "epoch": 0.476878612716763,
      "grad_norm": 0.09836672991514206,
      "learning_rate": 0.0004525325615050651,
      "loss": 0.2486,
      "step": 330
    },
    {
      "epoch": 0.4913294797687861,
      "grad_norm": 17.22054100036621,
      "learning_rate": 0.00045108538350217077,
      "loss": 0.3056,
      "step": 340
    },
    {
      "epoch": 0.5057803468208093,
      "grad_norm": 2.32596755027771,
      "learning_rate": 0.00044963820549927643,
      "loss": 0.2993,
      "step": 350
    },
    {
      "epoch": 0.5202312138728323,
      "grad_norm": 13.823657035827637,
      "learning_rate": 0.0004481910274963821,
      "loss": 0.388,
      "step": 360
    },
    {
      "epoch": 0.5346820809248555,
      "grad_norm": 4.803829669952393,
      "learning_rate": 0.00044674384949348775,
      "loss": 0.3115,
      "step": 370
    },
    {
      "epoch": 0.5491329479768786,
      "grad_norm": 11.129261016845703,
      "learning_rate": 0.00044529667149059335,
      "loss": 0.3952,
      "step": 380
    },
    {
      "epoch": 0.5635838150289018,
      "grad_norm": 11.100576400756836,
      "learning_rate": 0.0004439942112879884,
      "loss": 0.4997,
      "step": 390
    },
    {
      "epoch": 0.5780346820809249,
      "grad_norm": 8.661707878112793,
      "learning_rate": 0.0004425470332850941,
      "loss": 0.2506,
      "step": 400
    },
    {
      "epoch": 0.5924855491329479,
      "grad_norm": 0.050359439104795456,
      "learning_rate": 0.0004410998552821997,
      "loss": 0.3472,
      "step": 410
    },
    {
      "epoch": 0.6069364161849711,
      "grad_norm": 22.49631118774414,
      "learning_rate": 0.00043965267727930535,
      "loss": 0.5262,
      "step": 420
    },
    {
      "epoch": 0.6213872832369942,
      "grad_norm": 15.580559730529785,
      "learning_rate": 0.000438205499276411,
      "loss": 0.4131,
      "step": 430
    },
    {
      "epoch": 0.6358381502890174,
      "grad_norm": 0.05586950108408928,
      "learning_rate": 0.00043675832127351667,
      "loss": 0.3409,
      "step": 440
    },
    {
      "epoch": 0.6502890173410405,
      "grad_norm": 16.91840171813965,
      "learning_rate": 0.00043531114327062233,
      "loss": 0.5235,
      "step": 450
    },
    {
      "epoch": 0.6647398843930635,
      "grad_norm": 14.609116554260254,
      "learning_rate": 0.00043386396526772793,
      "loss": 0.3477,
      "step": 460
    },
    {
      "epoch": 0.6791907514450867,
      "grad_norm": 7.137592792510986,
      "learning_rate": 0.0004324167872648336,
      "loss": 0.2628,
      "step": 470
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 24.277603149414062,
      "learning_rate": 0.00043096960926193925,
      "loss": 0.604,
      "step": 480
    },
    {
      "epoch": 0.708092485549133,
      "grad_norm": 0.051298003643751144,
      "learning_rate": 0.0004295224312590449,
      "loss": 0.2455,
      "step": 490
    },
    {
      "epoch": 0.7225433526011561,
      "grad_norm": 10.662684440612793,
      "learning_rate": 0.0004280752532561505,
      "loss": 0.4071,
      "step": 500
    },
    {
      "epoch": 0.7369942196531792,
      "grad_norm": 0.003882271470502019,
      "learning_rate": 0.0004266280752532561,
      "loss": 0.1809,
      "step": 510
    },
    {
      "epoch": 0.7514450867052023,
      "grad_norm": 10.307286262512207,
      "learning_rate": 0.0004251808972503618,
      "loss": 0.2764,
      "step": 520
    },
    {
      "epoch": 0.7658959537572254,
      "grad_norm": 16.534849166870117,
      "learning_rate": 0.00042373371924746744,
      "loss": 0.5069,
      "step": 530
    },
    {
      "epoch": 0.7803468208092486,
      "grad_norm": 12.39257526397705,
      "learning_rate": 0.0004222865412445731,
      "loss": 0.3879,
      "step": 540
    },
    {
      "epoch": 0.7947976878612717,
      "grad_norm": 9.95926570892334,
      "learning_rate": 0.0004208393632416787,
      "loss": 0.4177,
      "step": 550
    },
    {
      "epoch": 0.8092485549132948,
      "grad_norm": 6.62819766998291,
      "learning_rate": 0.00041939218523878437,
      "loss": 0.2433,
      "step": 560
    },
    {
      "epoch": 0.8236994219653179,
      "grad_norm": 11.272332191467285,
      "learning_rate": 0.00041794500723589,
      "loss": 0.2783,
      "step": 570
    },
    {
      "epoch": 0.838150289017341,
      "grad_norm": 5.606941223144531,
      "learning_rate": 0.0004164978292329957,
      "loss": 0.3024,
      "step": 580
    },
    {
      "epoch": 0.8526011560693642,
      "grad_norm": 15.641378402709961,
      "learning_rate": 0.00041505065123010134,
      "loss": 0.378,
      "step": 590
    },
    {
      "epoch": 0.8670520231213873,
      "grad_norm": 5.2149176597595215,
      "learning_rate": 0.00041360347322720695,
      "loss": 0.4238,
      "step": 600
    },
    {
      "epoch": 0.8815028901734104,
      "grad_norm": 22.05489158630371,
      "learning_rate": 0.0004121562952243126,
      "loss": 0.5466,
      "step": 610
    },
    {
      "epoch": 0.8959537572254336,
      "grad_norm": 14.45259952545166,
      "learning_rate": 0.0004107091172214182,
      "loss": 0.3459,
      "step": 620
    },
    {
      "epoch": 0.9104046242774566,
      "grad_norm": 16.362085342407227,
      "learning_rate": 0.0004092619392185239,
      "loss": 0.4956,
      "step": 630
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 12.74441146850586,
      "learning_rate": 0.00040781476121562953,
      "loss": 0.2495,
      "step": 640
    },
    {
      "epoch": 0.9393063583815029,
      "grad_norm": 6.119001865386963,
      "learning_rate": 0.00040636758321273514,
      "loss": 0.4375,
      "step": 650
    },
    {
      "epoch": 0.953757225433526,
      "grad_norm": 8.401885986328125,
      "learning_rate": 0.0004049204052098408,
      "loss": 0.6333,
      "step": 660
    },
    {
      "epoch": 0.9682080924855492,
      "grad_norm": 10.825050354003906,
      "learning_rate": 0.00040347322720694646,
      "loss": 0.5856,
      "step": 670
    },
    {
      "epoch": 0.9826589595375722,
      "grad_norm": 3.767749071121216,
      "learning_rate": 0.0004020260492040521,
      "loss": 0.4744,
      "step": 680
    },
    {
      "epoch": 0.9971098265895953,
      "grad_norm": 12.295722007751465,
      "learning_rate": 0.0004005788712011578,
      "loss": 0.3254,
      "step": 690
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3624313771724701,
      "eval_runtime": 14.4445,
      "eval_samples_per_second": 23.4,
      "eval_steps_per_second": 11.7,
      "step": 692
    },
    {
      "epoch": 1.0115606936416186,
      "grad_norm": 0.054234180599451065,
      "learning_rate": 0.0003991316931982634,
      "loss": 0.2629,
      "step": 700
    },
    {
      "epoch": 1.0260115606936415,
      "grad_norm": 8.816060066223145,
      "learning_rate": 0.00039768451519536904,
      "loss": 0.3648,
      "step": 710
    },
    {
      "epoch": 1.0404624277456647,
      "grad_norm": 14.626583099365234,
      "learning_rate": 0.0003962373371924747,
      "loss": 0.3425,
      "step": 720
    },
    {
      "epoch": 1.0549132947976878,
      "grad_norm": 14.21831226348877,
      "learning_rate": 0.0003947901591895803,
      "loss": 0.3972,
      "step": 730
    },
    {
      "epoch": 1.069364161849711,
      "grad_norm": 7.934236526489258,
      "learning_rate": 0.00039334298118668597,
      "loss": 0.3645,
      "step": 740
    },
    {
      "epoch": 1.083815028901734,
      "grad_norm": 12.307042121887207,
      "learning_rate": 0.00039189580318379157,
      "loss": 0.2658,
      "step": 750
    },
    {
      "epoch": 1.0982658959537572,
      "grad_norm": 15.515406608581543,
      "learning_rate": 0.00039044862518089723,
      "loss": 0.3073,
      "step": 760
    },
    {
      "epoch": 1.1127167630057804,
      "grad_norm": 11.462160110473633,
      "learning_rate": 0.0003890014471780029,
      "loss": 0.3129,
      "step": 770
    },
    {
      "epoch": 1.1271676300578035,
      "grad_norm": 9.5941801071167,
      "learning_rate": 0.00038755426917510855,
      "loss": 0.385,
      "step": 780
    },
    {
      "epoch": 1.1416184971098267,
      "grad_norm": 0.008978353813290596,
      "learning_rate": 0.0003861070911722142,
      "loss": 0.1137,
      "step": 790
    },
    {
      "epoch": 1.1560693641618498,
      "grad_norm": 18.149639129638672,
      "learning_rate": 0.0003846599131693198,
      "loss": 0.283,
      "step": 800
    },
    {
      "epoch": 1.1705202312138727,
      "grad_norm": 2.987577199935913,
      "learning_rate": 0.0003832127351664255,
      "loss": 0.2705,
      "step": 810
    },
    {
      "epoch": 1.1849710982658959,
      "grad_norm": 15.97552490234375,
      "learning_rate": 0.00038176555716353113,
      "loss": 0.3473,
      "step": 820
    },
    {
      "epoch": 1.199421965317919,
      "grad_norm": 0.0706574022769928,
      "learning_rate": 0.0003803183791606368,
      "loss": 0.1711,
      "step": 830
    },
    {
      "epoch": 1.2138728323699421,
      "grad_norm": 10.015910148620605,
      "learning_rate": 0.00037887120115774245,
      "loss": 0.3177,
      "step": 840
    },
    {
      "epoch": 1.2283236994219653,
      "grad_norm": 0.9610705971717834,
      "learning_rate": 0.000377424023154848,
      "loss": 0.1336,
      "step": 850
    },
    {
      "epoch": 1.2427745664739884,
      "grad_norm": 12.55492115020752,
      "learning_rate": 0.00037597684515195366,
      "loss": 0.3258,
      "step": 860
    },
    {
      "epoch": 1.2572254335260116,
      "grad_norm": 10.554644584655762,
      "learning_rate": 0.0003745296671490593,
      "loss": 0.2198,
      "step": 870
    },
    {
      "epoch": 1.2716763005780347,
      "grad_norm": 7.2254133224487305,
      "learning_rate": 0.000373082489146165,
      "loss": 0.3333,
      "step": 880
    },
    {
      "epoch": 1.2861271676300579,
      "grad_norm": 9.777438163757324,
      "learning_rate": 0.00037163531114327064,
      "loss": 0.4446,
      "step": 890
    },
    {
      "epoch": 1.300578034682081,
      "grad_norm": 6.263314723968506,
      "learning_rate": 0.00037018813314037625,
      "loss": 0.3434,
      "step": 900
    },
    {
      "epoch": 1.3150289017341041,
      "grad_norm": 8.967373847961426,
      "learning_rate": 0.0003687409551374819,
      "loss": 0.3168,
      "step": 910
    },
    {
      "epoch": 1.3294797687861273,
      "grad_norm": 0.04954071715474129,
      "learning_rate": 0.00036729377713458757,
      "loss": 0.2164,
      "step": 920
    },
    {
      "epoch": 1.3439306358381504,
      "grad_norm": 1.530339002609253,
      "learning_rate": 0.0003658465991316932,
      "loss": 0.2661,
      "step": 930
    },
    {
      "epoch": 1.3583815028901733,
      "grad_norm": 9.265365600585938,
      "learning_rate": 0.0003643994211287989,
      "loss": 0.3145,
      "step": 940
    },
    {
      "epoch": 1.3728323699421965,
      "grad_norm": 12.163724899291992,
      "learning_rate": 0.0003629522431259045,
      "loss": 0.3693,
      "step": 950
    },
    {
      "epoch": 1.3872832369942196,
      "grad_norm": 8.251282691955566,
      "learning_rate": 0.00036150506512301015,
      "loss": 0.2448,
      "step": 960
    },
    {
      "epoch": 1.4017341040462428,
      "grad_norm": 0.012557064183056355,
      "learning_rate": 0.00036005788712011576,
      "loss": 0.1397,
      "step": 970
    },
    {
      "epoch": 1.416184971098266,
      "grad_norm": 15.005369186401367,
      "learning_rate": 0.0003586107091172214,
      "loss": 0.2943,
      "step": 980
    },
    {
      "epoch": 1.430635838150289,
      "grad_norm": 7.487553596496582,
      "learning_rate": 0.0003571635311143271,
      "loss": 0.351,
      "step": 990
    },
    {
      "epoch": 1.4450867052023122,
      "grad_norm": 17.69911003112793,
      "learning_rate": 0.0003557163531114327,
      "loss": 0.3641,
      "step": 1000
    },
    {
      "epoch": 1.4595375722543353,
      "grad_norm": 5.079265117645264,
      "learning_rate": 0.00035426917510853834,
      "loss": 0.1444,
      "step": 1010
    },
    {
      "epoch": 1.4739884393063583,
      "grad_norm": 7.4118571281433105,
      "learning_rate": 0.000352821997105644,
      "loss": 0.3788,
      "step": 1020
    },
    {
      "epoch": 1.4884393063583814,
      "grad_norm": 0.015985427424311638,
      "learning_rate": 0.00035137481910274966,
      "loss": 0.2666,
      "step": 1030
    },
    {
      "epoch": 1.5028901734104045,
      "grad_norm": 10.062135696411133,
      "learning_rate": 0.0003499276410998553,
      "loss": 0.2295,
      "step": 1040
    },
    {
      "epoch": 1.5173410404624277,
      "grad_norm": 11.79397964477539,
      "learning_rate": 0.0003484804630969609,
      "loss": 0.191,
      "step": 1050
    },
    {
      "epoch": 1.5317919075144508,
      "grad_norm": 0.041338685899972916,
      "learning_rate": 0.0003470332850940666,
      "loss": 0.2988,
      "step": 1060
    },
    {
      "epoch": 1.546242774566474,
      "grad_norm": 2.925755500793457,
      "learning_rate": 0.00034558610709117224,
      "loss": 0.3652,
      "step": 1070
    },
    {
      "epoch": 1.560693641618497,
      "grad_norm": 0.02931515872478485,
      "learning_rate": 0.0003441389290882779,
      "loss": 0.1315,
      "step": 1080
    },
    {
      "epoch": 1.5751445086705202,
      "grad_norm": 10.301201820373535,
      "learning_rate": 0.0003426917510853835,
      "loss": 0.4116,
      "step": 1090
    },
    {
      "epoch": 1.5895953757225434,
      "grad_norm": 1.9679921865463257,
      "learning_rate": 0.0003412445730824891,
      "loss": 0.1853,
      "step": 1100
    },
    {
      "epoch": 1.6040462427745665,
      "grad_norm": 6.519735813140869,
      "learning_rate": 0.00033979739507959477,
      "loss": 0.1433,
      "step": 1110
    },
    {
      "epoch": 1.6184971098265897,
      "grad_norm": 0.001242493512108922,
      "learning_rate": 0.00033835021707670043,
      "loss": 0.2147,
      "step": 1120
    },
    {
      "epoch": 1.6329479768786128,
      "grad_norm": 16.094871520996094,
      "learning_rate": 0.0003369030390738061,
      "loss": 0.4273,
      "step": 1130
    },
    {
      "epoch": 1.647398843930636,
      "grad_norm": 1.023640751838684,
      "learning_rate": 0.00033545586107091175,
      "loss": 0.3386,
      "step": 1140
    },
    {
      "epoch": 1.661849710982659,
      "grad_norm": 9.2776517868042,
      "learning_rate": 0.00033400868306801735,
      "loss": 0.3503,
      "step": 1150
    },
    {
      "epoch": 1.6763005780346822,
      "grad_norm": 10.428861618041992,
      "learning_rate": 0.000332561505065123,
      "loss": 0.3199,
      "step": 1160
    },
    {
      "epoch": 1.6907514450867052,
      "grad_norm": 12.429630279541016,
      "learning_rate": 0.0003311143270622287,
      "loss": 0.2683,
      "step": 1170
    },
    {
      "epoch": 1.7052023121387283,
      "grad_norm": 7.637120723724365,
      "learning_rate": 0.00032966714905933433,
      "loss": 0.3608,
      "step": 1180
    },
    {
      "epoch": 1.7196531791907514,
      "grad_norm": 15.09961223602295,
      "learning_rate": 0.00032821997105644,
      "loss": 0.3555,
      "step": 1190
    },
    {
      "epoch": 1.7341040462427746,
      "grad_norm": 0.06915534287691116,
      "learning_rate": 0.00032677279305354554,
      "loss": 0.1903,
      "step": 1200
    },
    {
      "epoch": 1.7485549132947977,
      "grad_norm": 10.552998542785645,
      "learning_rate": 0.0003253256150506512,
      "loss": 0.3413,
      "step": 1210
    },
    {
      "epoch": 1.7630057803468207,
      "grad_norm": 0.013426223769783974,
      "learning_rate": 0.00032387843704775686,
      "loss": 0.1794,
      "step": 1220
    },
    {
      "epoch": 1.7774566473988438,
      "grad_norm": 9.257905960083008,
      "learning_rate": 0.0003224312590448625,
      "loss": 0.2636,
      "step": 1230
    },
    {
      "epoch": 1.791907514450867,
      "grad_norm": 6.96334171295166,
      "learning_rate": 0.0003209840810419682,
      "loss": 0.4823,
      "step": 1240
    },
    {
      "epoch": 1.80635838150289,
      "grad_norm": 8.52114486694336,
      "learning_rate": 0.0003195369030390738,
      "loss": 0.2665,
      "step": 1250
    },
    {
      "epoch": 1.8208092485549132,
      "grad_norm": 10.614872932434082,
      "learning_rate": 0.00031808972503617945,
      "loss": 0.3552,
      "step": 1260
    },
    {
      "epoch": 1.8352601156069364,
      "grad_norm": 2.9418091773986816,
      "learning_rate": 0.0003166425470332851,
      "loss": 0.273,
      "step": 1270
    },
    {
      "epoch": 1.8497109826589595,
      "grad_norm": 12.20544719696045,
      "learning_rate": 0.00031519536903039077,
      "loss": 0.2211,
      "step": 1280
    },
    {
      "epoch": 1.8641618497109826,
      "grad_norm": 0.017956610769033432,
      "learning_rate": 0.0003137481910274964,
      "loss": 0.2055,
      "step": 1290
    },
    {
      "epoch": 1.8786127167630058,
      "grad_norm": 3.5689218044281006,
      "learning_rate": 0.00031230101302460203,
      "loss": 0.2094,
      "step": 1300
    },
    {
      "epoch": 1.893063583815029,
      "grad_norm": 9.351677894592285,
      "learning_rate": 0.0003108538350217077,
      "loss": 0.3523,
      "step": 1310
    },
    {
      "epoch": 1.907514450867052,
      "grad_norm": 13.819421768188477,
      "learning_rate": 0.0003094066570188133,
      "loss": 0.237,
      "step": 1320
    },
    {
      "epoch": 1.9219653179190752,
      "grad_norm": 0.07843656092882156,
      "learning_rate": 0.00030795947901591895,
      "loss": 0.2595,
      "step": 1330
    },
    {
      "epoch": 1.9364161849710984,
      "grad_norm": 11.09911060333252,
      "learning_rate": 0.0003065123010130246,
      "loss": 0.1803,
      "step": 1340
    },
    {
      "epoch": 1.9508670520231215,
      "grad_norm": 0.010673973709344864,
      "learning_rate": 0.0003050651230101302,
      "loss": 0.2884,
      "step": 1350
    },
    {
      "epoch": 1.9653179190751446,
      "grad_norm": 17.534299850463867,
      "learning_rate": 0.0003036179450072359,
      "loss": 0.3147,
      "step": 1360
    },
    {
      "epoch": 1.9797687861271678,
      "grad_norm": 11.901906967163086,
      "learning_rate": 0.00030217076700434154,
      "loss": 0.2122,
      "step": 1370
    },
    {
      "epoch": 1.9942196531791907,
      "grad_norm": 0.028397655114531517,
      "learning_rate": 0.0003007235890014472,
      "loss": 0.2793,
      "step": 1380
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.31865525245666504,
      "eval_runtime": 14.4418,
      "eval_samples_per_second": 23.404,
      "eval_steps_per_second": 11.702,
      "step": 1384
    },
    {
      "epoch": 2.008670520231214,
      "grad_norm": 8.541631698608398,
      "learning_rate": 0.00029927641099855286,
      "loss": 0.3228,
      "step": 1390
    },
    {
      "epoch": 2.023121387283237,
      "grad_norm": 12.840656280517578,
      "learning_rate": 0.00029782923299565846,
      "loss": 0.1493,
      "step": 1400
    },
    {
      "epoch": 2.03757225433526,
      "grad_norm": 10.56161117553711,
      "learning_rate": 0.0002963820549927641,
      "loss": 0.2197,
      "step": 1410
    },
    {
      "epoch": 2.052023121387283,
      "grad_norm": 17.179855346679688,
      "learning_rate": 0.0002949348769898698,
      "loss": 0.334,
      "step": 1420
    },
    {
      "epoch": 2.066473988439306,
      "grad_norm": 9.936488151550293,
      "learning_rate": 0.00029348769898697544,
      "loss": 0.3737,
      "step": 1430
    },
    {
      "epoch": 2.0809248554913293,
      "grad_norm": 11.688339233398438,
      "learning_rate": 0.00029204052098408105,
      "loss": 0.1686,
      "step": 1440
    },
    {
      "epoch": 2.0953757225433525,
      "grad_norm": 2.8038947582244873,
      "learning_rate": 0.00029059334298118665,
      "loss": 0.2299,
      "step": 1450
    },
    {
      "epoch": 2.1098265895953756,
      "grad_norm": 7.27433967590332,
      "learning_rate": 0.0002891461649782923,
      "loss": 0.1564,
      "step": 1460
    },
    {
      "epoch": 2.1242774566473988,
      "grad_norm": 8.729623794555664,
      "learning_rate": 0.00028769898697539797,
      "loss": 0.1873,
      "step": 1470
    },
    {
      "epoch": 2.138728323699422,
      "grad_norm": 6.956781387329102,
      "learning_rate": 0.00028625180897250363,
      "loss": 0.1417,
      "step": 1480
    },
    {
      "epoch": 2.153179190751445,
      "grad_norm": 0.017959924414753914,
      "learning_rate": 0.0002848046309696093,
      "loss": 0.1175,
      "step": 1490
    },
    {
      "epoch": 2.167630057803468,
      "grad_norm": 12.579556465148926,
      "learning_rate": 0.0002833574529667149,
      "loss": 0.1341,
      "step": 1500
    },
    {
      "epoch": 2.1820809248554913,
      "grad_norm": 0.049075670540332794,
      "learning_rate": 0.00028191027496382055,
      "loss": 0.1602,
      "step": 1510
    },
    {
      "epoch": 2.1965317919075145,
      "grad_norm": 14.818319320678711,
      "learning_rate": 0.0002804630969609262,
      "loss": 0.4544,
      "step": 1520
    },
    {
      "epoch": 2.2109826589595376,
      "grad_norm": 11.759785652160645,
      "learning_rate": 0.0002790159189580319,
      "loss": 0.2018,
      "step": 1530
    },
    {
      "epoch": 2.2254335260115607,
      "grad_norm": 1.6727252006530762,
      "learning_rate": 0.00027756874095513753,
      "loss": 0.2791,
      "step": 1540
    },
    {
      "epoch": 2.239884393063584,
      "grad_norm": 8.527863502502441,
      "learning_rate": 0.0002761215629522431,
      "loss": 0.1996,
      "step": 1550
    },
    {
      "epoch": 2.254335260115607,
      "grad_norm": 0.4312804043292999,
      "learning_rate": 0.00027467438494934874,
      "loss": 0.1727,
      "step": 1560
    },
    {
      "epoch": 2.26878612716763,
      "grad_norm": 0.0022178301587700844,
      "learning_rate": 0.0002732272069464544,
      "loss": 0.1457,
      "step": 1570
    },
    {
      "epoch": 2.2832369942196533,
      "grad_norm": 3.75107479095459,
      "learning_rate": 0.00027178002894356006,
      "loss": 0.1203,
      "step": 1580
    },
    {
      "epoch": 2.2976878612716765,
      "grad_norm": 11.853082656860352,
      "learning_rate": 0.0002703328509406657,
      "loss": 0.389,
      "step": 1590
    },
    {
      "epoch": 2.3121387283236996,
      "grad_norm": 1.32276451587677,
      "learning_rate": 0.00026888567293777133,
      "loss": 0.1661,
      "step": 1600
    },
    {
      "epoch": 2.3265895953757223,
      "grad_norm": 9.0895357131958,
      "learning_rate": 0.000267438494934877,
      "loss": 0.1233,
      "step": 1610
    },
    {
      "epoch": 2.3410404624277454,
      "grad_norm": 11.185148239135742,
      "learning_rate": 0.00026599131693198265,
      "loss": 0.2315,
      "step": 1620
    },
    {
      "epoch": 2.3554913294797686,
      "grad_norm": 13.802565574645996,
      "learning_rate": 0.0002645441389290883,
      "loss": 0.2264,
      "step": 1630
    },
    {
      "epoch": 2.3699421965317917,
      "grad_norm": 0.0018058167770504951,
      "learning_rate": 0.00026309696092619397,
      "loss": 0.1291,
      "step": 1640
    },
    {
      "epoch": 2.384393063583815,
      "grad_norm": 16.912456512451172,
      "learning_rate": 0.00026164978292329957,
      "loss": 0.2418,
      "step": 1650
    },
    {
      "epoch": 2.398843930635838,
      "grad_norm": 12.871992111206055,
      "learning_rate": 0.00026020260492040523,
      "loss": 0.3866,
      "step": 1660
    },
    {
      "epoch": 2.413294797687861,
      "grad_norm": 10.597294807434082,
      "learning_rate": 0.00025875542691751084,
      "loss": 0.1977,
      "step": 1670
    },
    {
      "epoch": 2.4277456647398843,
      "grad_norm": 13.695569038391113,
      "learning_rate": 0.0002573082489146165,
      "loss": 0.322,
      "step": 1680
    },
    {
      "epoch": 2.4421965317919074,
      "grad_norm": 0.0556684173643589,
      "learning_rate": 0.00025586107091172215,
      "loss": 0.2791,
      "step": 1690
    },
    {
      "epoch": 2.4566473988439306,
      "grad_norm": 8.845587730407715,
      "learning_rate": 0.00025441389290882776,
      "loss": 0.084,
      "step": 1700
    },
    {
      "epoch": 2.4710982658959537,
      "grad_norm": 10.738044738769531,
      "learning_rate": 0.0002529667149059334,
      "loss": 0.0966,
      "step": 1710
    },
    {
      "epoch": 2.485549132947977,
      "grad_norm": 2.631834030151367,
      "learning_rate": 0.0002515195369030391,
      "loss": 0.1899,
      "step": 1720
    },
    {
      "epoch": 2.5,
      "grad_norm": 2.8010780811309814,
      "learning_rate": 0.00025007235890014474,
      "loss": 0.0576,
      "step": 1730
    },
    {
      "epoch": 2.514450867052023,
      "grad_norm": 0.4696974456310272,
      "learning_rate": 0.00024862518089725034,
      "loss": 0.2588,
      "step": 1740
    },
    {
      "epoch": 2.5289017341040463,
      "grad_norm": 0.2842634916305542,
      "learning_rate": 0.000247178002894356,
      "loss": 0.1306,
      "step": 1750
    },
    {
      "epoch": 2.5433526011560694,
      "grad_norm": 8.604750633239746,
      "learning_rate": 0.00024573082489146166,
      "loss": 0.4312,
      "step": 1760
    },
    {
      "epoch": 2.5578034682080926,
      "grad_norm": 0.1947757601737976,
      "learning_rate": 0.0002442836468885673,
      "loss": 0.1532,
      "step": 1770
    },
    {
      "epoch": 2.5722543352601157,
      "grad_norm": 0.2910762131214142,
      "learning_rate": 0.00024283646888567295,
      "loss": 0.2013,
      "step": 1780
    },
    {
      "epoch": 2.586705202312139,
      "grad_norm": 2.2250559329986572,
      "learning_rate": 0.0002413892908827786,
      "loss": 0.2714,
      "step": 1790
    },
    {
      "epoch": 2.601156069364162,
      "grad_norm": 1.7418549060821533,
      "learning_rate": 0.00023994211287988425,
      "loss": 0.1117,
      "step": 1800
    },
    {
      "epoch": 2.615606936416185,
      "grad_norm": 10.55768871307373,
      "learning_rate": 0.00023849493487698988,
      "loss": 0.1054,
      "step": 1810
    },
    {
      "epoch": 2.6300578034682083,
      "grad_norm": 2.1069979667663574,
      "learning_rate": 0.0002370477568740955,
      "loss": 0.2771,
      "step": 1820
    },
    {
      "epoch": 2.6445086705202314,
      "grad_norm": 0.09607020020484924,
      "learning_rate": 0.00023560057887120114,
      "loss": 0.2504,
      "step": 1830
    },
    {
      "epoch": 2.6589595375722546,
      "grad_norm": 10.633252143859863,
      "learning_rate": 0.0002341534008683068,
      "loss": 0.2819,
      "step": 1840
    },
    {
      "epoch": 2.6734104046242777,
      "grad_norm": 0.10280882567167282,
      "learning_rate": 0.00023270622286541246,
      "loss": 0.1228,
      "step": 1850
    },
    {
      "epoch": 2.687861271676301,
      "grad_norm": 0.06716879457235336,
      "learning_rate": 0.0002312590448625181,
      "loss": 0.2485,
      "step": 1860
    },
    {
      "epoch": 2.7023121387283235,
      "grad_norm": 9.637993812561035,
      "learning_rate": 0.00022981186685962375,
      "loss": 0.2307,
      "step": 1870
    },
    {
      "epoch": 2.7167630057803467,
      "grad_norm": 7.846950054168701,
      "learning_rate": 0.00022836468885672936,
      "loss": 0.3925,
      "step": 1880
    },
    {
      "epoch": 2.73121387283237,
      "grad_norm": 0.03318294882774353,
      "learning_rate": 0.00022691751085383502,
      "loss": 0.2821,
      "step": 1890
    },
    {
      "epoch": 2.745664739884393,
      "grad_norm": 0.14386069774627686,
      "learning_rate": 0.00022547033285094068,
      "loss": 0.228,
      "step": 1900
    },
    {
      "epoch": 2.760115606936416,
      "grad_norm": 8.685840606689453,
      "learning_rate": 0.0002240231548480463,
      "loss": 0.2027,
      "step": 1910
    },
    {
      "epoch": 2.7745664739884393,
      "grad_norm": 3.151165246963501,
      "learning_rate": 0.00022257597684515197,
      "loss": 0.261,
      "step": 1920
    },
    {
      "epoch": 2.7890173410404624,
      "grad_norm": 0.00903507973998785,
      "learning_rate": 0.00022112879884225758,
      "loss": 0.3143,
      "step": 1930
    },
    {
      "epoch": 2.8034682080924855,
      "grad_norm": 6.4521331787109375,
      "learning_rate": 0.00021968162083936324,
      "loss": 0.114,
      "step": 1940
    },
    {
      "epoch": 2.8179190751445087,
      "grad_norm": 0.0007780584855936468,
      "learning_rate": 0.0002182344428364689,
      "loss": 0.17,
      "step": 1950
    },
    {
      "epoch": 2.832369942196532,
      "grad_norm": 0.003255009651184082,
      "learning_rate": 0.00021678726483357453,
      "loss": 0.1246,
      "step": 1960
    },
    {
      "epoch": 2.846820809248555,
      "grad_norm": 15.069978713989258,
      "learning_rate": 0.0002153400868306802,
      "loss": 0.2557,
      "step": 1970
    },
    {
      "epoch": 2.861271676300578,
      "grad_norm": 1.1236439943313599,
      "learning_rate": 0.00021389290882778582,
      "loss": 0.1555,
      "step": 1980
    },
    {
      "epoch": 2.8757225433526012,
      "grad_norm": 9.03347110748291,
      "learning_rate": 0.00021244573082489145,
      "loss": 0.2787,
      "step": 1990
    },
    {
      "epoch": 2.8901734104046244,
      "grad_norm": 12.652661323547363,
      "learning_rate": 0.0002109985528219971,
      "loss": 0.1568,
      "step": 2000
    },
    {
      "epoch": 2.9046242774566475,
      "grad_norm": 1.1519325971603394,
      "learning_rate": 0.00020955137481910274,
      "loss": 0.1615,
      "step": 2010
    },
    {
      "epoch": 2.9190751445086707,
      "grad_norm": 0.07549575716257095,
      "learning_rate": 0.0002081041968162084,
      "loss": 0.1583,
      "step": 2020
    },
    {
      "epoch": 2.9335260115606934,
      "grad_norm": 8.190727233886719,
      "learning_rate": 0.00020665701881331404,
      "loss": 0.2565,
      "step": 2030
    },
    {
      "epoch": 2.9479768786127165,
      "grad_norm": 8.836159706115723,
      "learning_rate": 0.0002052098408104197,
      "loss": 0.276,
      "step": 2040
    },
    {
      "epoch": 2.9624277456647397,
      "grad_norm": 9.176487922668457,
      "learning_rate": 0.00020376266280752533,
      "loss": 0.1611,
      "step": 2050
    },
    {
      "epoch": 2.976878612716763,
      "grad_norm": 8.443944931030273,
      "learning_rate": 0.00020231548480463096,
      "loss": 0.2749,
      "step": 2060
    },
    {
      "epoch": 2.991329479768786,
      "grad_norm": 6.861740589141846,
      "learning_rate": 0.00020086830680173662,
      "loss": 0.1829,
      "step": 2070
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.3031710982322693,
      "eval_runtime": 14.4107,
      "eval_samples_per_second": 23.455,
      "eval_steps_per_second": 11.727,
      "step": 2076
    },
    {
      "epoch": 3.005780346820809,
      "grad_norm": 0.4921129047870636,
      "learning_rate": 0.00019942112879884225,
      "loss": 0.1824,
      "step": 2080
    },
    {
      "epoch": 3.020231213872832,
      "grad_norm": 11.265609741210938,
      "learning_rate": 0.0001979739507959479,
      "loss": 0.0766,
      "step": 2090
    },
    {
      "epoch": 3.0346820809248554,
      "grad_norm": 10.589446067810059,
      "learning_rate": 0.00019652677279305357,
      "loss": 0.1472,
      "step": 2100
    },
    {
      "epoch": 3.0491329479768785,
      "grad_norm": 4.287625312805176,
      "learning_rate": 0.00019507959479015918,
      "loss": 0.1535,
      "step": 2110
    },
    {
      "epoch": 3.0635838150289016,
      "grad_norm": 10.392000198364258,
      "learning_rate": 0.00019363241678726484,
      "loss": 0.1961,
      "step": 2120
    },
    {
      "epoch": 3.078034682080925,
      "grad_norm": 0.4521743953227997,
      "learning_rate": 0.00019218523878437047,
      "loss": 0.1556,
      "step": 2130
    },
    {
      "epoch": 3.092485549132948,
      "grad_norm": 6.404841423034668,
      "learning_rate": 0.00019073806078147613,
      "loss": 0.1772,
      "step": 2140
    },
    {
      "epoch": 3.106936416184971,
      "grad_norm": 0.10331078618764877,
      "learning_rate": 0.0001892908827785818,
      "loss": 0.1254,
      "step": 2150
    },
    {
      "epoch": 3.121387283236994,
      "grad_norm": 0.017840547487139702,
      "learning_rate": 0.00018784370477568742,
      "loss": 0.1842,
      "step": 2160
    },
    {
      "epoch": 3.1358381502890174,
      "grad_norm": 5.064628601074219,
      "learning_rate": 0.00018639652677279305,
      "loss": 0.0892,
      "step": 2170
    },
    {
      "epoch": 3.1502890173410405,
      "grad_norm": 15.738520622253418,
      "learning_rate": 0.00018494934876989868,
      "loss": 0.176,
      "step": 2180
    },
    {
      "epoch": 3.1647398843930636,
      "grad_norm": 12.629751205444336,
      "learning_rate": 0.00018350217076700434,
      "loss": 0.2303,
      "step": 2190
    },
    {
      "epoch": 3.179190751445087,
      "grad_norm": 4.155923843383789,
      "learning_rate": 0.00018205499276411,
      "loss": 0.0849,
      "step": 2200
    },
    {
      "epoch": 3.19364161849711,
      "grad_norm": 11.948542594909668,
      "learning_rate": 0.00018060781476121564,
      "loss": 0.1031,
      "step": 2210
    },
    {
      "epoch": 3.208092485549133,
      "grad_norm": 14.108271598815918,
      "learning_rate": 0.0001791606367583213,
      "loss": 0.1368,
      "step": 2220
    },
    {
      "epoch": 3.222543352601156,
      "grad_norm": 2.793987989425659,
      "learning_rate": 0.0001777134587554269,
      "loss": 0.137,
      "step": 2230
    },
    {
      "epoch": 3.2369942196531793,
      "grad_norm": 0.056741517037153244,
      "learning_rate": 0.00017626628075253256,
      "loss": 0.1382,
      "step": 2240
    },
    {
      "epoch": 3.2514450867052025,
      "grad_norm": 2.3020219802856445,
      "learning_rate": 0.00017481910274963822,
      "loss": 0.0796,
      "step": 2250
    },
    {
      "epoch": 3.2658959537572256,
      "grad_norm": 1.5946499109268188,
      "learning_rate": 0.00017337192474674385,
      "loss": 0.1307,
      "step": 2260
    },
    {
      "epoch": 3.2803468208092488,
      "grad_norm": 1.0372689962387085,
      "learning_rate": 0.0001719247467438495,
      "loss": 0.0727,
      "step": 2270
    },
    {
      "epoch": 3.294797687861272,
      "grad_norm": 4.8250861167907715,
      "learning_rate": 0.00017047756874095514,
      "loss": 0.0975,
      "step": 2280
    },
    {
      "epoch": 3.3092485549132946,
      "grad_norm": 3.0180089473724365,
      "learning_rate": 0.00016903039073806078,
      "loss": 0.0925,
      "step": 2290
    },
    {
      "epoch": 3.3236994219653178,
      "grad_norm": 7.159355163574219,
      "learning_rate": 0.00016758321273516644,
      "loss": 0.1884,
      "step": 2300
    },
    {
      "epoch": 3.338150289017341,
      "grad_norm": 0.06290660053491592,
      "learning_rate": 0.00016613603473227207,
      "loss": 0.1127,
      "step": 2310
    },
    {
      "epoch": 3.352601156069364,
      "grad_norm": 5.082202911376953,
      "learning_rate": 0.00016468885672937773,
      "loss": 0.1582,
      "step": 2320
    },
    {
      "epoch": 3.367052023121387,
      "grad_norm": 7.152046203613281,
      "learning_rate": 0.00016324167872648336,
      "loss": 0.063,
      "step": 2330
    },
    {
      "epoch": 3.3815028901734103,
      "grad_norm": 3.402155637741089,
      "learning_rate": 0.000161794500723589,
      "loss": 0.0487,
      "step": 2340
    },
    {
      "epoch": 3.3959537572254335,
      "grad_norm": 8.86892032623291,
      "learning_rate": 0.00016034732272069465,
      "loss": 0.1553,
      "step": 2350
    },
    {
      "epoch": 3.4104046242774566,
      "grad_norm": 7.757384777069092,
      "learning_rate": 0.00015890014471780028,
      "loss": 0.073,
      "step": 2360
    },
    {
      "epoch": 3.4248554913294798,
      "grad_norm": 0.004006735049188137,
      "learning_rate": 0.00015745296671490594,
      "loss": 0.0842,
      "step": 2370
    },
    {
      "epoch": 3.439306358381503,
      "grad_norm": 10.755027770996094,
      "learning_rate": 0.00015600578871201158,
      "loss": 0.1786,
      "step": 2380
    },
    {
      "epoch": 3.453757225433526,
      "grad_norm": 0.3854203522205353,
      "learning_rate": 0.00015455861070911724,
      "loss": 0.1066,
      "step": 2390
    },
    {
      "epoch": 3.468208092485549,
      "grad_norm": 2.5785911083221436,
      "learning_rate": 0.00015311143270622287,
      "loss": 0.0863,
      "step": 2400
    },
    {
      "epoch": 3.4826589595375723,
      "grad_norm": 6.374592304229736,
      "learning_rate": 0.0001516642547033285,
      "loss": 0.0522,
      "step": 2410
    },
    {
      "epoch": 3.4971098265895955,
      "grad_norm": 10.808493614196777,
      "learning_rate": 0.00015021707670043416,
      "loss": 0.1238,
      "step": 2420
    },
    {
      "epoch": 3.5115606936416186,
      "grad_norm": 0.6618020534515381,
      "learning_rate": 0.0001487698986975398,
      "loss": 0.1157,
      "step": 2430
    },
    {
      "epoch": 3.5260115606936417,
      "grad_norm": 0.044452570378780365,
      "learning_rate": 0.00014732272069464545,
      "loss": 0.0376,
      "step": 2440
    },
    {
      "epoch": 3.540462427745665,
      "grad_norm": 0.05309125408530235,
      "learning_rate": 0.0001458755426917511,
      "loss": 0.1049,
      "step": 2450
    },
    {
      "epoch": 3.5549132947976876,
      "grad_norm": 7.669594764709473,
      "learning_rate": 0.00014442836468885672,
      "loss": 0.0745,
      "step": 2460
    },
    {
      "epoch": 3.5693641618497107,
      "grad_norm": 9.532365798950195,
      "learning_rate": 0.00014298118668596238,
      "loss": 0.102,
      "step": 2470
    },
    {
      "epoch": 3.583815028901734,
      "grad_norm": 4.028080940246582,
      "learning_rate": 0.000141534008683068,
      "loss": 0.0435,
      "step": 2480
    },
    {
      "epoch": 3.598265895953757,
      "grad_norm": 7.811445713043213,
      "learning_rate": 0.00014008683068017367,
      "loss": 0.1076,
      "step": 2490
    },
    {
      "epoch": 3.61271676300578,
      "grad_norm": 8.39851188659668,
      "learning_rate": 0.00013863965267727933,
      "loss": 0.1895,
      "step": 2500
    },
    {
      "epoch": 3.6271676300578033,
      "grad_norm": 2.828535318374634,
      "learning_rate": 0.00013719247467438496,
      "loss": 0.1142,
      "step": 2510
    },
    {
      "epoch": 3.6416184971098264,
      "grad_norm": 4.13322639465332,
      "learning_rate": 0.0001357452966714906,
      "loss": 0.2384,
      "step": 2520
    },
    {
      "epoch": 3.6560693641618496,
      "grad_norm": 0.006372860632836819,
      "learning_rate": 0.00013429811866859622,
      "loss": 0.0444,
      "step": 2530
    },
    {
      "epoch": 3.6705202312138727,
      "grad_norm": 8.806259155273438,
      "learning_rate": 0.00013285094066570188,
      "loss": 0.1864,
      "step": 2540
    },
    {
      "epoch": 3.684971098265896,
      "grad_norm": 13.477448463439941,
      "learning_rate": 0.00013140376266280754,
      "loss": 0.1217,
      "step": 2550
    },
    {
      "epoch": 3.699421965317919,
      "grad_norm": 10.070959091186523,
      "learning_rate": 0.00012995658465991318,
      "loss": 0.1966,
      "step": 2560
    },
    {
      "epoch": 3.713872832369942,
      "grad_norm": 1.8581185340881348,
      "learning_rate": 0.00012850940665701884,
      "loss": 0.195,
      "step": 2570
    },
    {
      "epoch": 3.7283236994219653,
      "grad_norm": 7.340474605560303,
      "learning_rate": 0.00012706222865412444,
      "loss": 0.1656,
      "step": 2580
    },
    {
      "epoch": 3.7427745664739884,
      "grad_norm": 8.425771713256836,
      "learning_rate": 0.0001256150506512301,
      "loss": 0.2522,
      "step": 2590
    },
    {
      "epoch": 3.7572254335260116,
      "grad_norm": 0.12024304270744324,
      "learning_rate": 0.00012416787264833576,
      "loss": 0.1853,
      "step": 2600
    },
    {
      "epoch": 3.7716763005780347,
      "grad_norm": 0.4247155487537384,
      "learning_rate": 0.0001227206946454414,
      "loss": 0.119,
      "step": 2610
    },
    {
      "epoch": 3.786127167630058,
      "grad_norm": 14.495100975036621,
      "learning_rate": 0.00012127351664254702,
      "loss": 0.1624,
      "step": 2620
    },
    {
      "epoch": 3.800578034682081,
      "grad_norm": 0.025572916492819786,
      "learning_rate": 0.00011982633863965268,
      "loss": 0.0592,
      "step": 2630
    },
    {
      "epoch": 3.815028901734104,
      "grad_norm": 0.017934642732143402,
      "learning_rate": 0.00011837916063675833,
      "loss": 0.0891,
      "step": 2640
    },
    {
      "epoch": 3.8294797687861273,
      "grad_norm": 11.685839653015137,
      "learning_rate": 0.00011693198263386396,
      "loss": 0.13,
      "step": 2650
    },
    {
      "epoch": 3.8439306358381504,
      "grad_norm": 0.36503857374191284,
      "learning_rate": 0.00011548480463096961,
      "loss": 0.0888,
      "step": 2660
    },
    {
      "epoch": 3.8583815028901736,
      "grad_norm": 4.700315475463867,
      "learning_rate": 0.00011403762662807525,
      "loss": 0.1907,
      "step": 2670
    },
    {
      "epoch": 3.8728323699421967,
      "grad_norm": 8.848175048828125,
      "learning_rate": 0.0001125904486251809,
      "loss": 0.1278,
      "step": 2680
    },
    {
      "epoch": 3.88728323699422,
      "grad_norm": 8.323080062866211,
      "learning_rate": 0.00011114327062228655,
      "loss": 0.055,
      "step": 2690
    },
    {
      "epoch": 3.901734104046243,
      "grad_norm": 0.0034504267387092113,
      "learning_rate": 0.00010969609261939219,
      "loss": 0.1007,
      "step": 2700
    },
    {
      "epoch": 3.916184971098266,
      "grad_norm": 0.3692331314086914,
      "learning_rate": 0.00010824891461649782,
      "loss": 0.0848,
      "step": 2710
    },
    {
      "epoch": 3.9306358381502893,
      "grad_norm": 1.3390064239501953,
      "learning_rate": 0.00010680173661360347,
      "loss": 0.0662,
      "step": 2720
    },
    {
      "epoch": 3.9450867052023124,
      "grad_norm": 12.138195037841797,
      "learning_rate": 0.00010535455861070913,
      "loss": 0.1353,
      "step": 2730
    },
    {
      "epoch": 3.959537572254335,
      "grad_norm": 0.05925862491130829,
      "learning_rate": 0.00010390738060781476,
      "loss": 0.2328,
      "step": 2740
    },
    {
      "epoch": 3.9739884393063583,
      "grad_norm": 0.5735036134719849,
      "learning_rate": 0.00010246020260492041,
      "loss": 0.1336,
      "step": 2750
    },
    {
      "epoch": 3.9884393063583814,
      "grad_norm": 1.128933072090149,
      "learning_rate": 0.00010101302460202605,
      "loss": 0.1044,
      "step": 2760
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.3220411241054535,
      "eval_runtime": 14.3774,
      "eval_samples_per_second": 23.509,
      "eval_steps_per_second": 11.755,
      "step": 2768
    },
    {
      "epoch": 4.002890173410405,
      "grad_norm": 1.759401798248291,
      "learning_rate": 9.956584659913169e-05,
      "loss": 0.0443,
      "step": 2770
    },
    {
      "epoch": 4.017341040462428,
      "grad_norm": 0.3850070536136627,
      "learning_rate": 9.811866859623735e-05,
      "loss": 0.1252,
      "step": 2780
    },
    {
      "epoch": 4.031791907514451,
      "grad_norm": 0.8107109665870667,
      "learning_rate": 9.667149059334299e-05,
      "loss": 0.0163,
      "step": 2790
    },
    {
      "epoch": 4.046242774566474,
      "grad_norm": 4.510538578033447,
      "learning_rate": 9.522431259044862e-05,
      "loss": 0.1352,
      "step": 2800
    },
    {
      "epoch": 4.0606936416184976,
      "grad_norm": 7.627812385559082,
      "learning_rate": 9.377713458755427e-05,
      "loss": 0.0663,
      "step": 2810
    },
    {
      "epoch": 4.07514450867052,
      "grad_norm": 3.1556692123413086,
      "learning_rate": 9.232995658465992e-05,
      "loss": 0.0458,
      "step": 2820
    },
    {
      "epoch": 4.089595375722543,
      "grad_norm": 0.008849890902638435,
      "learning_rate": 9.088277858176556e-05,
      "loss": 0.0864,
      "step": 2830
    },
    {
      "epoch": 4.104046242774566,
      "grad_norm": 3.6288297176361084,
      "learning_rate": 8.943560057887121e-05,
      "loss": 0.0606,
      "step": 2840
    },
    {
      "epoch": 4.118497109826589,
      "grad_norm": 3.1301827430725098,
      "learning_rate": 8.798842257597685e-05,
      "loss": 0.044,
      "step": 2850
    },
    {
      "epoch": 4.132947976878612,
      "grad_norm": 0.920871376991272,
      "learning_rate": 8.654124457308249e-05,
      "loss": 0.0478,
      "step": 2860
    },
    {
      "epoch": 4.1473988439306355,
      "grad_norm": 0.0022165789268910885,
      "learning_rate": 8.509406657018813e-05,
      "loss": 0.0387,
      "step": 2870
    },
    {
      "epoch": 4.161849710982659,
      "grad_norm": 3.554367780685425,
      "learning_rate": 8.364688856729379e-05,
      "loss": 0.0458,
      "step": 2880
    },
    {
      "epoch": 4.176300578034682,
      "grad_norm": 1.7620729207992554,
      "learning_rate": 8.219971056439942e-05,
      "loss": 0.0476,
      "step": 2890
    },
    {
      "epoch": 4.190751445086705,
      "grad_norm": 0.005382247269153595,
      "learning_rate": 8.075253256150507e-05,
      "loss": 0.0562,
      "step": 2900
    },
    {
      "epoch": 4.205202312138728,
      "grad_norm": 10.216655731201172,
      "learning_rate": 7.930535455861072e-05,
      "loss": 0.0873,
      "step": 2910
    },
    {
      "epoch": 4.219653179190751,
      "grad_norm": 7.201388835906982,
      "learning_rate": 7.785817655571635e-05,
      "loss": 0.0564,
      "step": 2920
    },
    {
      "epoch": 4.234104046242774,
      "grad_norm": 0.5705186724662781,
      "learning_rate": 7.6410998552822e-05,
      "loss": 0.0736,
      "step": 2930
    },
    {
      "epoch": 4.2485549132947975,
      "grad_norm": 0.014134944416582584,
      "learning_rate": 7.496382054992764e-05,
      "loss": 0.0331,
      "step": 2940
    },
    {
      "epoch": 4.263005780346821,
      "grad_norm": 0.1782781481742859,
      "learning_rate": 7.351664254703329e-05,
      "loss": 0.036,
      "step": 2950
    },
    {
      "epoch": 4.277456647398844,
      "grad_norm": 3.6298980712890625,
      "learning_rate": 7.206946454413893e-05,
      "loss": 0.0591,
      "step": 2960
    },
    {
      "epoch": 4.291907514450867,
      "grad_norm": 7.4673686027526855,
      "learning_rate": 7.062228654124456e-05,
      "loss": 0.0298,
      "step": 2970
    },
    {
      "epoch": 4.30635838150289,
      "grad_norm": 0.0617855079472065,
      "learning_rate": 6.917510853835021e-05,
      "loss": 0.1247,
      "step": 2980
    },
    {
      "epoch": 4.320809248554913,
      "grad_norm": 0.004385831765830517,
      "learning_rate": 6.772793053545587e-05,
      "loss": 0.0884,
      "step": 2990
    },
    {
      "epoch": 4.335260115606936,
      "grad_norm": 1.244157075881958,
      "learning_rate": 6.62807525325615e-05,
      "loss": 0.0191,
      "step": 3000
    },
    {
      "epoch": 4.3497109826589595,
      "grad_norm": 0.06420933455228806,
      "learning_rate": 6.483357452966715e-05,
      "loss": 0.0364,
      "step": 3010
    },
    {
      "epoch": 4.364161849710983,
      "grad_norm": 3.7122130393981934,
      "learning_rate": 6.33863965267728e-05,
      "loss": 0.103,
      "step": 3020
    },
    {
      "epoch": 4.378612716763006,
      "grad_norm": 0.04706885665655136,
      "learning_rate": 6.193921852387844e-05,
      "loss": 0.0673,
      "step": 3030
    },
    {
      "epoch": 4.393063583815029,
      "grad_norm": 0.1445048302412033,
      "learning_rate": 6.049204052098408e-05,
      "loss": 0.0342,
      "step": 3040
    },
    {
      "epoch": 4.407514450867052,
      "grad_norm": 3.870375871658325,
      "learning_rate": 5.9044862518089725e-05,
      "loss": 0.046,
      "step": 3050
    },
    {
      "epoch": 4.421965317919075,
      "grad_norm": 0.3424398601055145,
      "learning_rate": 5.759768451519537e-05,
      "loss": 0.1336,
      "step": 3060
    },
    {
      "epoch": 4.436416184971098,
      "grad_norm": 0.10141704976558685,
      "learning_rate": 5.615050651230101e-05,
      "loss": 0.0115,
      "step": 3070
    },
    {
      "epoch": 4.4508670520231215,
      "grad_norm": 0.4328724443912506,
      "learning_rate": 5.4703328509406656e-05,
      "loss": 0.0369,
      "step": 3080
    },
    {
      "epoch": 4.465317919075145,
      "grad_norm": 0.010221761651337147,
      "learning_rate": 5.32561505065123e-05,
      "loss": 0.1104,
      "step": 3090
    },
    {
      "epoch": 4.479768786127168,
      "grad_norm": 3.4155473709106445,
      "learning_rate": 5.180897250361795e-05,
      "loss": 0.0367,
      "step": 3100
    },
    {
      "epoch": 4.494219653179191,
      "grad_norm": 3.5692191123962402,
      "learning_rate": 5.036179450072359e-05,
      "loss": 0.1102,
      "step": 3110
    },
    {
      "epoch": 4.508670520231214,
      "grad_norm": 0.5900048017501831,
      "learning_rate": 4.8914616497829233e-05,
      "loss": 0.0043,
      "step": 3120
    },
    {
      "epoch": 4.523121387283237,
      "grad_norm": 5.221939563751221,
      "learning_rate": 4.746743849493488e-05,
      "loss": 0.0764,
      "step": 3130
    },
    {
      "epoch": 4.53757225433526,
      "grad_norm": 0.010839731432497501,
      "learning_rate": 4.602026049204052e-05,
      "loss": 0.0387,
      "step": 3140
    },
    {
      "epoch": 4.5520231213872835,
      "grad_norm": 12.742664337158203,
      "learning_rate": 4.457308248914617e-05,
      "loss": 0.0609,
      "step": 3150
    },
    {
      "epoch": 4.566473988439307,
      "grad_norm": 1.411225438117981,
      "learning_rate": 4.312590448625181e-05,
      "loss": 0.0891,
      "step": 3160
    },
    {
      "epoch": 4.58092485549133,
      "grad_norm": 0.3696226477622986,
      "learning_rate": 4.167872648335745e-05,
      "loss": 0.0748,
      "step": 3170
    },
    {
      "epoch": 4.595375722543353,
      "grad_norm": 0.08949998021125793,
      "learning_rate": 4.02315484804631e-05,
      "loss": 0.0163,
      "step": 3180
    },
    {
      "epoch": 4.609826589595376,
      "grad_norm": 0.22561486065387726,
      "learning_rate": 3.878437047756874e-05,
      "loss": 0.0567,
      "step": 3190
    },
    {
      "epoch": 4.624277456647399,
      "grad_norm": 0.5070422291755676,
      "learning_rate": 3.733719247467439e-05,
      "loss": 0.0043,
      "step": 3200
    },
    {
      "epoch": 4.638728323699422,
      "grad_norm": 0.002460416406393051,
      "learning_rate": 3.5890014471780033e-05,
      "loss": 0.0556,
      "step": 3210
    },
    {
      "epoch": 4.653179190751445,
      "grad_norm": 0.9741578698158264,
      "learning_rate": 3.444283646888567e-05,
      "loss": 0.0634,
      "step": 3220
    },
    {
      "epoch": 4.667630057803468,
      "grad_norm": 0.20365242660045624,
      "learning_rate": 3.299565846599132e-05,
      "loss": 0.0181,
      "step": 3230
    },
    {
      "epoch": 4.682080924855491,
      "grad_norm": 0.49230194091796875,
      "learning_rate": 3.154848046309696e-05,
      "loss": 0.0278,
      "step": 3240
    },
    {
      "epoch": 4.696531791907514,
      "grad_norm": 0.6638242602348328,
      "learning_rate": 3.0101302460202607e-05,
      "loss": 0.1278,
      "step": 3250
    },
    {
      "epoch": 4.710982658959537,
      "grad_norm": 0.0021194941364228725,
      "learning_rate": 2.865412445730825e-05,
      "loss": 0.0237,
      "step": 3260
    },
    {
      "epoch": 4.72543352601156,
      "grad_norm": 0.0012314475607126951,
      "learning_rate": 2.7206946454413896e-05,
      "loss": 0.0479,
      "step": 3270
    },
    {
      "epoch": 4.7398843930635834,
      "grad_norm": 0.17684273421764374,
      "learning_rate": 2.5759768451519538e-05,
      "loss": 0.0714,
      "step": 3280
    },
    {
      "epoch": 4.754335260115607,
      "grad_norm": 4.482603073120117,
      "learning_rate": 2.431259044862518e-05,
      "loss": 0.0955,
      "step": 3290
    },
    {
      "epoch": 4.76878612716763,
      "grad_norm": 0.0390305370092392,
      "learning_rate": 2.2865412445730827e-05,
      "loss": 0.0765,
      "step": 3300
    },
    {
      "epoch": 4.783236994219653,
      "grad_norm": 0.27277061343193054,
      "learning_rate": 2.141823444283647e-05,
      "loss": 0.028,
      "step": 3310
    },
    {
      "epoch": 4.797687861271676,
      "grad_norm": 11.362947463989258,
      "learning_rate": 1.9971056439942112e-05,
      "loss": 0.055,
      "step": 3320
    },
    {
      "epoch": 4.812138728323699,
      "grad_norm": 0.3188360631465912,
      "learning_rate": 1.8523878437047758e-05,
      "loss": 0.0244,
      "step": 3330
    },
    {
      "epoch": 4.826589595375722,
      "grad_norm": 0.014163462445139885,
      "learning_rate": 1.70767004341534e-05,
      "loss": 0.0934,
      "step": 3340
    },
    {
      "epoch": 4.841040462427745,
      "grad_norm": 1.6336771249771118,
      "learning_rate": 1.5629522431259046e-05,
      "loss": 0.0234,
      "step": 3350
    },
    {
      "epoch": 4.855491329479769,
      "grad_norm": 5.273839473724365,
      "learning_rate": 1.4182344428364689e-05,
      "loss": 0.0378,
      "step": 3360
    },
    {
      "epoch": 4.869942196531792,
      "grad_norm": 0.5873215198516846,
      "learning_rate": 1.2735166425470333e-05,
      "loss": 0.169,
      "step": 3370
    },
    {
      "epoch": 4.884393063583815,
      "grad_norm": 0.00405877735465765,
      "learning_rate": 1.1287988422575977e-05,
      "loss": 0.0136,
      "step": 3380
    },
    {
      "epoch": 4.898843930635838,
      "grad_norm": 0.003877422073855996,
      "learning_rate": 9.840810419681621e-06,
      "loss": 0.0185,
      "step": 3390
    },
    {
      "epoch": 4.913294797687861,
      "grad_norm": 0.07765599340200424,
      "learning_rate": 8.393632416787264e-06,
      "loss": 0.1219,
      "step": 3400
    },
    {
      "epoch": 4.927745664739884,
      "grad_norm": 0.018884969875216484,
      "learning_rate": 6.946454413892909e-06,
      "loss": 0.0678,
      "step": 3410
    },
    {
      "epoch": 4.942196531791907,
      "grad_norm": 0.8821984529495239,
      "learning_rate": 5.499276410998553e-06,
      "loss": 0.0535,
      "step": 3420
    },
    {
      "epoch": 4.956647398843931,
      "grad_norm": 0.19612251222133636,
      "learning_rate": 4.052098408104197e-06,
      "loss": 0.0356,
      "step": 3430
    },
    {
      "epoch": 4.971098265895954,
      "grad_norm": 0.005688563454896212,
      "learning_rate": 2.6049204052098406e-06,
      "loss": 0.1158,
      "step": 3440
    },
    {
      "epoch": 4.985549132947977,
      "grad_norm": 1.5191129446029663,
      "learning_rate": 1.1577424023154849e-06,
      "loss": 0.0054,
      "step": 3450
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.006371496710926294,
      "learning_rate": 0.0,
      "loss": 0.0234,
      "step": 3460
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.3377986550331116,
      "eval_runtime": 14.0982,
      "eval_samples_per_second": 23.975,
      "eval_steps_per_second": 11.987,
      "step": 3460
    }
  ],
  "logging_steps": 10,
  "max_steps": 3460,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.069113340952576e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
